{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sophiastiles/.pyenv/versions/3.11.11/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from lightly.transforms import SimCLRTransform\n",
    "from numpy.random import Generator\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from lightly.data import LightlyDataset\n",
    "from torch.optim import SGD\n",
    "from itertools import chain\n",
    "from models import ConvNet\n",
    "from lightly.models.modules.heads import SimCLRProjectionHead\n",
    "from lightly.loss import NTXentLoss\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from torchvision.datasets import MNIST\n",
    "from pandas import DataFrame\n",
    "import math\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sophiastiles/.pyenv/versions/3.11.11/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "dataset_class = partial(MNIST, root=Path(\"mnist_data\"), download=True)\n",
    "\n",
    "train_transform = SimCLRTransform(\n",
    "    input_size=28,\n",
    "    min_scale=0.5,\n",
    "    hf_prob=0,\n",
    "    rr_prob=0,\n",
    "    vf_prob=0,\n",
    "    normalize=dict(mean=[0.1307], std=[0.3081]),\n",
    ")\n",
    "\n",
    "test_transform = Compose([ToTensor(), Normalize(mean=[0.1307], std=[0.3081])])\n",
    "\n",
    "train_set_for_training = LightlyDataset.from_torch_dataset(dataset_class(train=True), transform=train_transform)\n",
    "train_loader_for_training = DataLoader(train_set_for_training, batch_size=1_024, shuffle=True, drop_last=True)\n",
    "\n",
    "# Step 3: Define the encoder (ConvNet)\n",
    "encoder = ConvNet(input_shape=[28, 28], output_size=128)\n",
    "encoder = encoder.to(device)\n",
    "\n",
    "# Step 4: Define the projection head (SimCLRProjectionHead)\n",
    "proj_head = SimCLRProjectionHead(input_dim=128, hidden_dim=512)\n",
    "proj_head = proj_head.to(device)\n",
    "\n",
    "# Step 5: Define the loss function (NTXentLoss)\n",
    "loss_fn = NTXentLoss()\n",
    "\n",
    "# Step 6: Set up the optimizer\n",
    "params = chain(encoder.parameters(), proj_head.parameters())\n",
    "optimizer = SGD(params, lr=1, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next(dataloader: DataLoader) -> Union[Tensor, Tuple]:\n",
    "    try:\n",
    "        return next(dataloader)\n",
    "    except:\n",
    "        dataloader = iter(dataloader)\n",
    "        return next(dataloader)\n",
    "\n",
    "def save_table(path: Union[Path, str], table: DataFrame, formatting: dict) -> None:\n",
    "    for key in formatting:\n",
    "        if key in table:\n",
    "            table[key] = table[key].apply(formatting[key])\n",
    "\n",
    "    table.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 7.482248306274414\n",
      "Step 1: loss = 7.605940341949463\n",
      "Step 2: loss = 7.674834251403809\n",
      "Step 3: loss = 7.77217435836792\n",
      "Step 4: loss = 7.623018741607666\n",
      "Step 5: loss = 7.376786708831787\n",
      "Step 6: loss = 7.392028331756592\n",
      "Step 7: loss = 7.34305477142334\n",
      "Step 8: loss = 6.99934720993042\n",
      "Step 9: loss = 7.099389553070068\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "steps, losses = [], []\n",
    "n_optim_steps = 10\n",
    "\n",
    "for step in range(n_optim_steps):\n",
    "    (inputs_0, inputs_1), _, _ = get_next(train_loader_for_training)\n",
    "\n",
    "    embeddings_0 = proj_head(encoder(inputs_0.to(device)))\n",
    "    embeddings_1 = proj_head(encoder(inputs_1.to(device)))\n",
    "\n",
    "    loss = loss_fn(embeddings_0, embeddings_1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    steps += [step]\n",
    "    losses += [loss.item()]\n",
    "    print(f\"Step {step}: loss = {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Saving results...\")\n",
    "# train_log = pd.DataFrame({\"step\": steps, \"loss\": losses})\n",
    "# formatting = {\"loss\": \"{:.4f}\".format}\n",
    "# save_table(\"train_encoders/pretraining.csv\", train_log, formatting)\n",
    "\n",
    "# torch.save(encoder.state_dict(), \"train_encoders/encoder.pth\")\n",
    "# torch.save(proj_head.state_dict(), \"train_encoders/projection_head.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** On subset train\n",
      "encoding step 0\n",
      "encoding step 10\n",
      "encoding step 20\n",
      "encoding step 30\n",
      "encoding step 40\n",
      "encoding step 50\n",
      "\n",
      "********** On subset test\n",
      "encoding step 0\n"
     ]
    }
   ],
   "source": [
    "@torch.inference_mode()\n",
    "def encode(loader: DataLoader, encoder: ConvNet, device: str) -> np.ndarray:\n",
    "    embeddings = []\n",
    "\n",
    "    for idx, (images_i, _) in enumerate(loader):\n",
    "        if idx % 10 == 0:\n",
    "            print(f\"encoding step {idx}\")\n",
    "        images_i = images_i.to(device)\n",
    "        embeddings_i = encoder(images_i)\n",
    "        embeddings += [embeddings_i.cpu().numpy()]\n",
    "    return np.concatenate(embeddings)\n",
    "\n",
    "max_array_size = int(2e7)\n",
    "# GitHub has a max file size of 100MB, which translates to 2.5e7 32-bit floats.\n",
    "embeddings_simclr_train, embeddings_simclr_test = None, None\n",
    "labels_train, labels_test = None, None\n",
    "for subset in (\"train\", \"test\"):\n",
    "    print(f\"\\n********** On subset {subset}\")\n",
    "    \n",
    "    # Create dataset and DataLoader for current subset\n",
    "    dataset = dataset_class(train=(subset == \"train\"), transform=test_transform)\n",
    "    loader = DataLoader(dataset, batch_size=1_024)\n",
    "\n",
    "    # Generate embeddings for the subset\n",
    "    embeddings = encode(loader, encoder, device)\n",
    "    if subset == \"train\":\n",
    "        embeddings_simclr_train = embeddings\n",
    "    else:\n",
    "        embeddings_simclr_test = embeddings\n",
    "\n",
    "    # Check if the embeddings size exceeds the max allowed size\n",
    "    if embeddings.size > max_array_size:\n",
    "        # Split the embeddings into smaller parts\n",
    "        n_splits = math.ceil(embeddings.size / max_array_size)\n",
    "\n",
    "        for i, embeddings_i in enumerate(np.array_split(embeddings, n_splits, axis=0)):\n",
    "            # Save each part to a separate file\n",
    "            filepath = f\"embeddings/simclr_{subset}_part{i + 1}of{n_splits}.npy\"\n",
    "            # np.save(filepath, embeddings_i, allow_pickle=False)\n",
    "    # else:\n",
    "        # Save the full embeddings if it's under the max size\n",
    "        # np.save(f\"embeddings/simclr_{subset}.npy\", embeddings, allow_pickle=False)\n",
    "\n",
    "    # Save the labels\n",
    "    if subset == \"train\":\n",
    "        labels_train = dataset.targets\n",
    "    else:\n",
    "        labels_test = dataset.targets\n",
    "    # np.save(f\"embeddings/labels_{subset}.npy\", dataset.targets, allow_pickle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalute Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rng(seed: int = -1) -> Generator:\n",
    "    \"\"\"\n",
    "    References:\n",
    "        https://pytorch.org/docs/stable/notes/randomness.html\n",
    "    \"\"\"\n",
    "    if seed == -1:\n",
    "        seed = random.randint(0, int(1e6))\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    return np.random.default_rng(seed)\n",
    "\n",
    "\n",
    "def compute_mean_and_std(\n",
    "    x: np.ndarray, axis: int = 0, keepdims: bool = True\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    To avoid dividing by zero we set the standard deviation to one if it is less than epsilon.\n",
    "    \"\"\"\n",
    "    mean = np.mean(x, axis=axis, keepdims=keepdims)\n",
    "    std = np.std(x, axis=axis, keepdims=keepdims)\n",
    "\n",
    "    eps = np.finfo(x.dtype).eps\n",
    "    if isinstance(std, np.ndarray):\n",
    "        std[std < eps] = 1\n",
    "    elif std < eps:\n",
    "        std = 1\n",
    "    return mean, std\n",
    "\n",
    "def sample_n_per_class(labels: np.ndarray, n_per_class: int, rng: Generator) -> np.ndarray:\n",
    "    sampled_inds = []\n",
    "\n",
    "    for _class in np.unique(labels):\n",
    "        class_inds = np.flatnonzero(labels == _class)\n",
    "        sampled_inds += [rng.choice(class_inds, size=n_per_class, replace=False)]\n",
    "\n",
    "    sampled_inds = np.concatenate(sampled_inds)\n",
    "    sampled_inds = rng.permutation(sampled_inds)\n",
    "\n",
    "    return sampled_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0    # TODO: change default seed \n",
    "rng = get_rng(seed)\n",
    "\n",
    "mean, std = compute_mean_and_std(embeddings_simclr_test)\n",
    "train_inputs = (embeddings_simclr_train - mean) / std\n",
    "test_inputs = (embeddings_simclr_test - mean) / std\n",
    "\n",
    "n_per_class, n_labels_end = 1, 500\n",
    "n_labels, accs, logliks = [], [], []\n",
    "\n",
    "while True:\n",
    "    train_inds = sample_n_per_class(labels_train, n_per_class, rng)\n",
    "    model = LogisticRegression(C=1, max_iter=1_000)\n",
    "    model.fit(train_inputs[train_inds], labels_train[train_inds])\n",
    "\n",
    "    probs = model.predict_proba(test_inputs)\n",
    "    acc = model.score(test_inputs, labels_test)\n",
    "    loglik = -log_loss(labels_test, probs)\n",
    "\n",
    "    n_labels += [len(train_inds)]\n",
    "    accs += [acc]\n",
    "    logliks += [loglik]\n",
    "\n",
    "    if len(train_inds) >= n_labels_end:\n",
    "        break\n",
    "    else:\n",
    "        n_per_class += 1\n",
    "\n",
    "test_log = pd.DataFrame({\"n_labels\": n_labels, \"test_acc\": accs, \"test_loglik\": logliks})\n",
    "formatting = {\n",
    "    \"n_labels\": \"{:03}\".format,\n",
    "    \"test_acc\": \"{:.4f}\".format,\n",
    "    \"test_loglik\": \"{:.4f}\".format,\n",
    "}\n",
    "# save_table(f\"results/testing_seed{seed}.csv\", test_log, formatting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(save_path: str) -> None:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot Test Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(n_labels, [acc * 100 for acc in accs])\n",
    "    plt.xlabel(\"Number of Labels\")\n",
    "    plt.ylabel(\"Test Accuracy (%)\")\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot Test Log Likelihood\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(n_labels, logliks)\n",
    "    plt.xlabel(\"Number of Labels\")\n",
    "    plt.ylabel(\"Test Expected Log Likelihood\")\n",
    "    plt.title(\"Expected Log Likelihood\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(\"plots/10_epochs__10k_iters.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
