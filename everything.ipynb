{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from subprocess import check_call\n",
    "from typing import Any, Callable, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from lightly.transforms import SimCLRTransform\n",
    "from numpy.lib.npyio import NpzFile\n",
    "from numpy.random import Generator\n",
    "from PIL import Image\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST, VisionDataset\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from lightly.data import LightlyDataset\n",
    "from torch import nn\n",
    "from torch.optim import SGD\n",
    "from itertools import chain\n",
    "from models import ConvNet\n",
    "from lightly.models.modules.heads import SimCLRProjectionHead\n",
    "from lightly.loss import NTXentLoss\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sophiastiles/.pyenv/versions/3.11.11/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "dataset_class = partial(MNIST, root=Path(\"mnist_data\"), download=True)\n",
    "\n",
    "train_transform = SimCLRTransform(\n",
    "    input_size=28,\n",
    "    min_scale=0.5,\n",
    "    hf_prob=0,\n",
    "    rr_prob=0,\n",
    "    vf_prob=0,\n",
    "    normalize=dict(mean=[0.1307], std=[0.3081]),\n",
    ")\n",
    "\n",
    "test_transform = Compose([ToTensor(), Normalize(mean=[0.1307], std=[0.3081])])\n",
    "\n",
    "train_dataset = dataset_class(train=True)\n",
    "\n",
    "# Step 1: Transform the train and test sets with the respective transformations\n",
    "train_set_for_training = LightlyDataset.from_torch_dataset(train_dataset, transform=train_transform)\n",
    "\n",
    "# Step 2: Create DataLoader for both train and test sets\n",
    "train_loader_for_training = DataLoader(train_set_for_training, batch_size=1_024, shuffle=True, drop_last=True)\n",
    "\n",
    "# Step 3: Define the encoder (ConvNet)\n",
    "encoder = ConvNet(input_shape=[28, 28], output_size=128)\n",
    "encoder = encoder.to(device)\n",
    "\n",
    "# Step 4: Define the projection head (SimCLRProjectionHead)\n",
    "proj_head = SimCLRProjectionHead(input_dim=128, hidden_dim=512)\n",
    "proj_head = proj_head.to(device)\n",
    "\n",
    "# Step 5: Define the loss function (NTXentLoss)\n",
    "loss_fn = NTXentLoss()\n",
    "\n",
    "# Step 6: Set up the optimizer\n",
    "params = chain(encoder.parameters(), proj_head.parameters())\n",
    "optimizer = SGD(params, lr=1, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "def get_next(dataloader: DataLoader) -> Union[Tensor, Tuple]:\n",
    "    try:\n",
    "        return next(dataloader)\n",
    "    except:\n",
    "        dataloader = iter(dataloader)\n",
    "        return next(dataloader)\n",
    "\n",
    "def save_table(path: Union[Path, str], table: DataFrame, formatting: dict) -> None:\n",
    "    for key in formatting:\n",
    "        if key in table:\n",
    "            table[key] = table[key].apply(formatting[key])\n",
    "\n",
    "    table.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 7.496316432952881\n",
      "Step 1: loss = 7.665528774261475\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(embeddings_0, embeddings_1)\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 15\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     18\u001b[0m steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [step]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "steps, losses = [], []\n",
    "n_optim_steps = 10\n",
    "\n",
    "for step in range(n_optim_steps):\n",
    "    (inputs_0, inputs_1), _, _ = get_next(train_loader_for_training)\n",
    "\n",
    "    embeddings_0 = proj_head(encoder(inputs_0.to(device)))\n",
    "    embeddings_1 = proj_head(encoder(inputs_1.to(device)))\n",
    "\n",
    "    loss = loss_fn(embeddings_0, embeddings_1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    steps += [step]\n",
    "    losses += [loss.item()]\n",
    "    print(f\"Step {step}: loss = {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results...\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving results...\")\n",
    "train_log = pd.DataFrame({\"step\": steps, \"loss\": losses})\n",
    "formatting = {\"loss\": \"{:.4f}\".format}\n",
    "save_table(\"train_encoders/pretraining.csv\", train_log, formatting)\n",
    "\n",
    "torch.save(encoder.state_dict(), \"train_encoders/encoder.pth\")\n",
    "torch.save(proj_head.state_dict(), \"train_encoders/projection_head.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** On subset train\n",
      "encoding step 0\n",
      "encoding step 10\n",
      "encoding step 20\n",
      "encoding step 30\n",
      "encoding step 40\n",
      "encoding step 50\n",
      "\n",
      "********** On subset test\n",
      "encoding step 0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "@torch.inference_mode()\n",
    "def encode(loader: DataLoader, encoder: ConvNet, device: str) -> np.ndarray:\n",
    "    embeddings = []\n",
    "\n",
    "    for idx, (images_i, _) in enumerate(loader):\n",
    "        if idx % 10 == 0:\n",
    "            print(f\"encoding step {idx}\")\n",
    "        images_i = images_i.to(device)\n",
    "        embeddings_i = encoder(images_i)\n",
    "        embeddings += [embeddings_i.cpu().numpy()]\n",
    "    return np.concatenate(embeddings)\n",
    "\n",
    "test_transform = Compose([ToTensor(), Normalize(mean=[0.1307], std=[0.3081])])\n",
    "\n",
    "train_set_for_embeddings = dataset_class(train=True, transform=test_transform)\n",
    "test_set_for_embeddings = dataset_class(train=False, transform=test_transform)\n",
    "\n",
    "# Step 2: Create DataLoader for both train and test sets\n",
    "train_loader_for_embeddings = DataLoader(train_set_for_embeddings, batch_size=1_024)\n",
    "test_loader_for_embeddings = DataLoader(test_set_for_embeddings, batch_size=1_024)\n",
    "\n",
    "max_array_size = int(2e7)\n",
    "# GitHub has a max file size of 100MB, which translates to 2.5e7 32-bit floats.\n",
    "\n",
    "for subset in (\"train\", \"test\"):\n",
    "    print(f\"\\n********** On subset {subset}\")\n",
    "    \n",
    "    # Create dataset and DataLoader for current subset\n",
    "    loader = train_loader_for_embeddings if subset == \"train\" else test_loader_for_embeddings\n",
    "\n",
    "    # Generate embeddings for the subset\n",
    "    embeddings = encode(loader, encoder, device)\n",
    "\n",
    "    # Check if the embeddings size exceeds the max allowed size\n",
    "    if embeddings.size > max_array_size:\n",
    "        # Split the embeddings into smaller parts\n",
    "        n_splits = math.ceil(embeddings.size / max_array_size)\n",
    "\n",
    "        for i, embeddings_i in enumerate(np.array_split(embeddings, n_splits, axis=0)):\n",
    "            # Save each part to a separate file\n",
    "            filepath = f\"embeddings/simclr_{subset}_part{i + 1}of{n_splits}.npy\"\n",
    "            np.save(filepath, embeddings_i, allow_pickle=False)\n",
    "    else:\n",
    "        # Save the full embeddings if it's under the max size\n",
    "        np.save(f\"embeddings/simclr_{subset}.npy\", embeddings, allow_pickle=False)\n",
    "\n",
    "    # Save the labels\n",
    "    saved_dataset = train_set_for_embeddings if subset == \"train\" else test_set_for_embeddings\n",
    "    np.save(f\"embeddings/labels_{subset}.npy\", saved_dataset.targets, allow_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
